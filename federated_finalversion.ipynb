{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescaMusella/MLDL-Project/blob/main/federated_finalversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEZP_zQu8D1e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "from torch import nn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "ag58Kd8cAtsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformation of CIFAR100\n",
        "transform = T.Compose([\n",
        "    T.Resize((32, 32)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "ytr65H_-AwgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "train_val=CIFAR100(root='.data/', train=True, download=True, transform=transform)\n",
        "test=CIFAR100(root='.data/', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "KX-DPKXeA-K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train-validation-test split\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "targets = train_val.targets\n",
        "\n",
        "train_indices, val_indices = train_test_split(\n",
        "    range(len(targets)),\n",
        "    test_size=0.1,\n",
        "    stratify=targets,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val = torch.utils.data.Subset(train_val, val_indices)\n",
        "\n",
        "val_loader= torch.utils.data.DataLoader(val, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "w3TyrGQjA_jZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IID dataset creation\n",
        "K=100\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "client_indices_list = []\n",
        "client_dataset=[]\n",
        "client_loader=[]\n",
        "client_loader_mask=[]\n",
        "\n",
        "targets = [train_val.targets[i] for i in train_indices]\n",
        "for _, client_idx in skf.split(train_indices, targets):\n",
        "    client_indices_list.append([train_indices[i] for i in client_idx])\n",
        "    client_dataset.append(torch.utils.data.Subset(train_val, client_indices_list[-1]))\n",
        "    client_loader.append(torch.utils.data.DataLoader(client_dataset[-1], batch_size=32, shuffle=True))\n",
        "\n",
        "    # Stratified split: take 15% of total client data maintaining class distribution\n",
        "    num_samples_mask = int(0.15 * len(client_dataset[-1]))\n",
        "    targets_im = [client_dataset[-1][i][1] for i in range(len(client_dataset[-1]))]\n",
        "    small_client_indices, _ = train_test_split(\n",
        "    range(len(client_dataset[-1])),\n",
        "    train_size=0.223,   #One image per class\n",
        "    stratify=targets_im,\n",
        "    random_state=42)\n",
        "\n",
        "    small_client = torch.utils.data.Subset(client_dataset[-1], small_client_indices)\n",
        "    small_client_loader = torch.utils.data.DataLoader(small_client, batch_size=1, shuffle=True)\n",
        "    client_loader_mask.append(small_client_loader)"
      ],
      "metadata": {
        "id": "iVtGvsD32I_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ViT-S/16\n",
        "!git clone https://github.com/facebookresearch/dino.git\n",
        "!ls"
      ],
      "metadata": {
        "id": "TJus-LYyn9VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "vits16_original = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', pretrained=True).to(device)\n",
        "vits16_new=copy.deepcopy(vits16_original)\n",
        "print(vits16_new)"
      ],
      "metadata": {
        "id": "u92WDQCDn-gM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change of the head and freezing of layers\n",
        "vits16_new.head = torch.nn.Linear(in_features=384,\n",
        "                    out_features=100,\n",
        "                    bias=True).to(device)\n",
        "\n",
        "for name, param in vits16_new.named_parameters():\n",
        "    if \"head\" not in name and \"patch_embed\" not in name and 'proj' not in name and 'pos_drop' not in name and 'attn' not in name:\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "vits16=copy.deepcopy(vits16_new)"
      ],
      "metadata": {
        "id": "Cb9CmqdAoJ5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FedAvg(model, K, C, J, T, lr, momentum, weight_decay, loss_fn, client_loader, trainable_part, implementation,  client_loader_mask, test_loader, test):\n",
        "  global_params = model.state_dict()\n",
        "\n",
        "  val_acc_fed_list=[]\n",
        "  val_loss_fed_list=[]\n",
        "\n",
        "  for t in range(T):\n",
        "      client_samples=random.sample(range(K), int(max(1, K*C)))\n",
        "      local_params = []\n",
        "      client_num_samples = []\n",
        "\n",
        "      for client in client_samples:\n",
        "          model_client=copy.deepcopy(model)\n",
        "          num_samples = len(client_loader[client].dataset)\n",
        "          client_num_samples.append(num_samples)\n",
        "\n",
        "          if trainable_part == 'head':\n",
        "              optimizer = torch.optim.SGD(model_client.head.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "          else:\n",
        "              optimizer = torch.optim.SGD(model_client.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "          if implementation=='self':\n",
        "                    sparsity=[0.1, 0.2, 0.3, 0.4, 0.66]\n",
        "                    mask_fed=compute_fisher_mask(model_client,  client_loader_mask[client], sparsity, loss_fn)\n",
        "\n",
        "          for loc_step in range(J):\n",
        "              batch_idx=torch.randint(0, len(client_loader[client]), (1,)).item()\n",
        "              for i, (inputs, labels) in enumerate(client_loader[client]):\n",
        "                  if i == batch_idx:\n",
        "                      inputs_client=inputs\n",
        "                      labels_client=labels\n",
        "                      break\n",
        "\n",
        "              if implementation=='self':\n",
        "                    train_sgd_sparse(loc_step, model_client, inputs_client, labels_client, loss_fn, lr, momentum, weight_decay, mask_fed)\n",
        "\n",
        "              else:\n",
        "                    train(loc_step, model_client, inputs_client, labels_client, loss_fn, optimizer,J,client)\n",
        "\n",
        "          local_params.append(model_client.state_dict())\n",
        "\n",
        "      total_samples = sum(client_num_samples)\n",
        "\n",
        "      new_global_params = copy.deepcopy(global_params)\n",
        "      for key in global_params.keys():\n",
        "            new_global_params[key] = sum(local_params[i][key].float() * (client_num_samples[i] / total_samples) for i in range(len(client_samples)))\n",
        "\n",
        "      model.load_state_dict(new_global_params)\n",
        "      global_params = new_global_params\n",
        "      if test:\n",
        "        val_acc_fed, val_loss_fed=validate(model, test_loader, loss_fn)\n",
        "      else:\n",
        "        val_acc_fed, val_loss_fed=validate(model, val_loader, loss_fn)\n",
        "\n",
        "      val_acc_fed_list.append(val_acc_fed)\n",
        "      val_loss_fed_list.append(val_loss_fed)\n",
        "\n",
        "      print(f'Iteration {t}-------------------')\n",
        "\n",
        "  return val_acc_fed_list, val_loss_fed_list"
      ],
      "metadata": {
        "id": "BByAM7ncQ76X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, model, inputs, targets, criterion, optimizer,J,client):\n",
        "    model.train()\n",
        "\n",
        "    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    intermediate_output = model.get_intermediate_layers(inputs, n=1)\n",
        "    features = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
        "    outputs = model.head(features)\n",
        "\n",
        "    loss=criterion(outputs, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch==J-1:\n",
        "        running_loss = loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total = targets.size(0)\n",
        "        correct = predicted.eq(targets).sum().item()\n",
        "\n",
        "        train_loss = running_loss\n",
        "        train_accuracy = 100. * correct / total\n",
        "        #print(f'Client: {client} Loss: {train_loss:.6f} Acc: {train_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "3t6CjNeqqdaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            intermediate_output = model.get_intermediate_layers(inputs, n=1)\n",
        "            features = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
        "            outputs = model.head(features)\n",
        "            loss=criterion(outputs, targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100. * correct / total\n",
        "\n",
        "    print(f'Validation Loss: {val_loss:.6f} Acc: {val_accuracy:.2f}%')\n",
        "    return val_accuracy, val_loss"
      ],
      "metadata": {
        "id": "wWvmAhbA9cLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vanilla FL with I.I.D sharding grid search\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "T = 80\n",
        "momentum = 0\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = 0\n",
        "best_param=[0, 0, 0]\n",
        "\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "weight_decay_values=[1e-3, 1e-4]\n",
        "\n",
        "lr_counter=0\n",
        "mt_counter=0\n",
        "x_acc=torch.zeros(len(learning_rates),len(weight_decay_values),T)\n",
        "x_loss=torch.zeros(len(learning_rates),len(weight_decay_values),T)\n",
        "for lr in learning_rates:\n",
        "    for weight_decay in weight_decay_values:\n",
        "        model=copy.deepcopy(vits16_new)\n",
        "        val_acc_fed_list, val_loss_fed_list=FedAvg(model, K, C, J, T, lr, momentum, weight_decay,loss_fn, client_loader, trainable_part='full', implementation='pyt',  client_loader_mask=client_loader_mask, test_loader=test_loader, test=False)\n",
        "        val_acc_fed, val_loss_fed=validate(model, val_loader, loss_fn)\n",
        "\n",
        "        if val_acc_fed > best_acc:\n",
        "            best_acc = val_acc_fed\n",
        "            best_param[0]=lr\n",
        "            best_param[1]=weight_decay\n",
        "\n",
        "\n",
        "        x_acc[lr_counter,mt_counter,:]=torch.tensor(val_acc_fed_list)\n",
        "        x_loss[lr_counter,mt_counter,:]=torch.tensor(val_loss_fed_list)\n",
        "\n",
        "        mt_counter+=1\n",
        "\n",
        "        print(\"-\"*30)\n",
        "    mt_counter=0\n",
        "    lr_counter+=1\n",
        "\n",
        "print(f'Best validation accuracy: {best_acc:.2f}%')\n",
        "print(f'Best learning rate: {best_param[0]}')\n",
        "print(f'Weight decay: {best_param[1]}')"
      ],
      "metadata": {
        "id": "xbN89eJRq63Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors=['green', 'orange']\n",
        "labels_lr=['lr=1e-3', 'lr=1e-4']\n",
        "\n",
        "ticks = [0] + [i for i in range(4, T, 5) if i != 0]\n",
        "labels = [1] + [i + 1 for i in range(4, T, 5) if i != 0]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots( 1, 2,figsize=(10,5))\n",
        "for i in range (0,2):\n",
        "  ax1.plot(x_loss[i,0,:], label = labels_lr[i], color=colors[i])\n",
        "ax1.set_xticks(ticks=ticks, labels=labels)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.grid(True)\n",
        "ax1.set_title('Val_loss Weight decay=1e-3')\n",
        "ax1.legend()\n",
        "for i in range (0,2):\n",
        "  ax2.plot(x_loss[i,1,:], label = labels_lr[i], color=colors[i])\n",
        "ax2.set_xticks(ticks=ticks, labels=labels)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.grid(True)\n",
        "ax2.set_title('Val_loss Weight decay=1e-4')\n",
        "ax2.legend()"
      ],
      "metadata": {
        "id": "d5xerTNxuG2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Vanilla FL with I.I.D sharding\n",
        "lr = 1e-3\n",
        "weight_decay=1e-4\n",
        "T=200\n",
        "vits16=copy.deepcopy(vits16_new)\n",
        "test_acc_fed_list, test_loss_fed_list=FedAvg(vits16, K, C, J, T, lr, momentum, weight_decay,loss_fn, client_loader, trainable_part='full', implementation='pyt',  client_loader_mask=client_loader_mask, test_loader=test_loader,test=True)"
      ],
      "metadata": {
        "id": "gvycSRQ7uUPL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 5))\n",
        "plt.plot(test_loss_fed_list, label='Test Loss')\n",
        "ticks = [0] + [i for i in range(4, T, 5) if i != 0]\n",
        "labels = [1] + [i + 1 for i in range(4, T, 5) if i != 0]\n",
        "plt.xticks(ticks=ticks, labels=labels)\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.title('Test loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O0ugC2ew3Ag3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#non I.I.D. not balanced dataset definition\n",
        "def create_dataset(N, K, num_class, train_indices, train_val):\n",
        "  class_client = np.zeros((K,N), dtype=int)\n",
        "\n",
        "  for i in range(K):\n",
        "    class_client[i,:]=random.sample(range(num_class), N)\n",
        "\n",
        "  unique_vals, nnzero_counts = np.unique(class_client, return_counts=True)\n",
        "\n",
        "  unique_counts = np.zeros(num_class, dtype=int)\n",
        "  for val, count in zip(unique_vals, nnzero_counts):\n",
        "    unique_counts[val] = count\n",
        "\n",
        "  class_to_indices = defaultdict(list)\n",
        "  for idx in train_indices:\n",
        "      label = train_val.targets[idx]\n",
        "      class_to_indices[label].append(idx)\n",
        "\n",
        "  for cls in class_to_indices:\n",
        "      random.shuffle(class_to_indices[cls])\n",
        "\n",
        "  client_indices_list = []\n",
        "  client_dataset = []\n",
        "  client_loader = []\n",
        "\n",
        "  client_indices_list_mask = []\n",
        "  client_dataset_mask = []\n",
        "  client_loader_mask = []\n",
        "\n",
        "  num_imm_per_class = int(len(train_indices)/(num_class))\n",
        "  numel_per_classclient = []\n",
        "  for cl in range(num_class):\n",
        "    if unique_counts[cl]==0:\n",
        "      numel_per_classclient.append(0)\n",
        "    else:\n",
        "      numel_per_classclient.append(int(num_imm_per_class/unique_counts[cl]))\n",
        "\n",
        "  for client_id in range(K):\n",
        "      class_client_list = class_client[client_id]\n",
        "      indices_client = []\n",
        "      indices_client_mask=[]\n",
        "\n",
        "      for cls in class_client_list:\n",
        "         indices_cls = class_to_indices[cls]\n",
        "         sampled = indices_cls[0:numel_per_classclient[cls]]\n",
        "\n",
        "         sampled_mask=indices_cls[0:max(1, int(0.15*numel_per_classclient[cls]))]\n",
        "\n",
        "         class_to_indices[cls] = class_to_indices[cls][numel_per_classclient[cls]:]\n",
        "         indices_client.extend(sampled)\n",
        "         indices_client_mask.extend(sampled_mask)\n",
        "\n",
        "      client_indices_list.append(indices_client)\n",
        "      client_indices_list_mask.append(indices_client_mask)\n",
        "      client_dataset.append(torch.utils.data.Subset(train_val, indices_client))\n",
        "      client_dataset_mask.append(torch.utils.data.Subset(train_val, indices_client_mask))\n",
        "      client_loader.append(torch.utils.data.DataLoader(client_dataset[-1], batch_size=32, shuffle=True))\n",
        "      client_loader_mask.append(torch.utils.data.DataLoader(client_dataset_mask[-1], batch_size=1, shuffle=True))\n",
        "\n",
        "  return client_indices_list,client_dataset,client_loader, client_loader_mask"
      ],
      "metadata": {
        "id": "Pyg5hD_EjZs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#non I.I.D balanced dataset definition\n",
        "def create_balanced_dataset(N, K, num_class, train_indices, train_val):\n",
        "   num_clients=K\n",
        "   num_class_client=N\n",
        "   mat_clients=np.full((num_clients,num_class_client), -1)\n",
        "   toll=0\n",
        "   nDisp=list(range(num_class))\n",
        "   nUtil= np.zeros(num_class, dtype=int)\n",
        "\n",
        "   for i in range(num_clients):\n",
        "     for j in range(num_class_client):\n",
        "        toll=random.choice(nDisp)\n",
        "        mat_clients[i,j]=toll\n",
        "        nDisp.remove(toll)\n",
        "        if len(nDisp)==0:\n",
        "          nDisp=list(range(num_class))\n",
        "\n",
        "   values, counts = np.unique(mat_clients, return_counts=True)\n",
        "\n",
        "   class_to_indices = defaultdict(list)\n",
        "   for idx in train_indices:\n",
        "      label = train_val.targets[idx]\n",
        "      class_to_indices[label].append(idx)\n",
        "\n",
        "   for cls in class_to_indices:\n",
        "      random.shuffle(class_to_indices[cls])\n",
        "\n",
        "   client_indices_list = []\n",
        "   client_indices_list_mask = []\n",
        "   client_dataset = []\n",
        "   client_dataset_mask=[]\n",
        "   client_loader = []\n",
        "   client_loader_mask=[]\n",
        "\n",
        "   numel_per_classclient = int(len(train_indices)/(num_class*N))\n",
        "\n",
        "   for client_id in range(K):\n",
        "      class_client = mat_clients[client_id]\n",
        "      indices_client = []\n",
        "      indices_client_mask=[]\n",
        "\n",
        "      for cls in class_client:\n",
        "         indices_cls = class_to_indices[cls]\n",
        "         sampled = indices_cls[0:numel_per_classclient]\n",
        "\n",
        "         sampled_mask=indices_cls[0:int(numel_per_classclient*0.15)]\n",
        "\n",
        "         class_to_indices[cls] = class_to_indices[cls][numel_per_classclient:]\n",
        "         indices_client.extend(sampled)\n",
        "\n",
        "         indices_client_mask.extend(sampled_mask)\n",
        "\n",
        "      client_indices_list.append(indices_client)\n",
        "      client_dataset.append(torch.utils.data.Subset(train_val, indices_client))\n",
        "      client_loader.append(torch.utils.data.DataLoader(client_dataset[-1], batch_size=32, shuffle=True))\n",
        "\n",
        "      client_indices_list_mask.append(indices_client_mask)\n",
        "      client_dataset_mask.append(torch.utils.data.Subset(train_val, indices_client_mask))\n",
        "      client_loader_mask.append(torch.utils.data.DataLoader(client_dataset_mask[-1], batch_size=1, shuffle=True))\n",
        "\n",
        "   return client_indices_list,client_dataset,client_loader, client_loader_mask"
      ],
      "metadata": {
        "id": "V4QxK6OFd_lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Control on the creation of dataset\n",
        "def control(train_val,client_indices_list,K):\n",
        "  for i in range(K):\n",
        "    targetforclient=[]\n",
        "    print(f\"Client {i}:\")\n",
        "    for im in client_indices_list[i]:\n",
        "      targetforclient.append(train_val.targets[im])\n",
        "    print(Counter(targetforclient))\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "  all_indices = [idx for client_indices in client_indices_list for idx in client_indices]\n",
        "  duplic = set(idx for idx in all_indices if all_indices.count(idx) > 1)\n",
        "  if duplic:\n",
        "    print(f\"Number of duplicated images found: {duplic}\")\n",
        "  else:\n",
        "    print(\"No duplicated images found.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Nm7yr0Of3DoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creation of non I.I.D. dataset\n",
        "Nc=[1,5,10,50]\n",
        "num_class=int(len(set(targets)))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = [4,8,16]\n",
        "T = 10\n",
        "lr = 1e-3\n",
        "momentum = 0\n",
        "weight_decay = 1e-3\n",
        "\n",
        "client_indices_list_noniid_balanced_list=[]\n",
        "client_dataset_noniid_balanced_list=[]\n",
        "client_loader_noniid_balanced_list=[]\n",
        "client_loader_mask_noniid_balanced_list=[]\n",
        "\n",
        "client_indices_list_noniid_list=[]\n",
        "client_dataset_noniid_list=[]\n",
        "client_loader_noniid_list=[]\n",
        "client_loader_mask_noniid_list=[]\n",
        "\n",
        "for N in Nc:\n",
        "  client_indices_list_noniid_balanced,client_dataset_noniid_balanced,client_loader_noniid_balanced, client_loader_mask_noniid_balanced=create_balanced_dataset(N, K, num_class, train_indices, train_val)\n",
        "  client_indices_list_noniid_balanced_list.append(client_indices_list_noniid_balanced)\n",
        "  client_dataset_noniid_balanced_list.append(client_dataset_noniid_balanced)\n",
        "  client_loader_noniid_balanced_list.append(client_loader_noniid_balanced)\n",
        "  client_loader_mask_noniid_balanced_list.append(client_loader_mask_noniid_balanced)\n",
        "\n",
        "  client_indices_list_noniid,client_dataset_noniid,client_loader_noniid, client_loader_mask_noniid=create_dataset(N,K, num_class,train_indices, train_val)\n",
        "  client_indices_list_noniid_list.append(client_indices_list_noniid)\n",
        "  client_dataset_noniid_list.append(client_dataset_noniid)\n",
        "  client_loader_noniid_list.append(client_loader_noniid)\n",
        "  client_loader_mask_noniid_list.append(client_loader_mask_noniid)"
      ],
      "metadata": {
        "id": "cB-pxQvhn6h7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vanilla FL with non I.I.D. balanced sharding for each J and Nc\n",
        "T=[200,100,50]\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-3\n",
        "for n in range(4):\n",
        "  for j in range(3):\n",
        "    vits16_nn_iid_balanced=copy.deepcopy(vits16_new)\n",
        "    FedAvg(vits16_nn_iid_balanced, K, C, J[j], T[j], lr, momentum, weight_decay,loss_fn, client_loader_noniid_balanced_list[n], trainable_part='full', implementation='pyt',  client_loader_mask=client_loader_mask_noniid_balanced_list[n], test_loader=test_loader, test=False)\n",
        "    print(f'Validation for non i.i.d. balanced dataset with N={Nc[n]} and J={J[j]}')\n",
        "    validate(vits16_nn_iid_balanced, val_loader, loss_fn)\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "642o2GOHTSz7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vanilla FL with non I.I.D. unbalanced data fixing J=4, Nc=50\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-3\n",
        "vits16_nn_iid=copy.deepcopy(vits16_new)\n",
        "FedAvg(vits16_nn_iid, K, C, 4, 200, lr, momentum, weight_decay,loss_fn, client_loader_noniid_list[3], trainable_part='full', implementation='pyt',  client_loader_mask=client_loader_mask_noniid_list[3], test_loader=test_loader, test=False)\n",
        "print(f'Validation for non i.i.d. dataset with with N={50} and J={4}')\n",
        "validate(vits16_nn_iid, val_loader, loss_fn)\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "pOzNoiIE1VOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creation of the mask\n",
        "def compute_fisher_mask(model, dataloader, sparsity, criterion):\n",
        "  fisher_scores = {}\n",
        "  prev_mask = {}\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for param in model.head.parameters():\n",
        "    param.requires_grad= False\n",
        "\n",
        "  for param in model.parameters():\n",
        "      if param.requires_grad:\n",
        "          fisher_scores[param] = torch.zeros_like(param.data)\n",
        "          prev_mask[param] = torch.ones_like(param.data)\n",
        "\n",
        "  for round in range(5):\n",
        "    for param in fisher_scores:\n",
        "        fisher_scores[param].zero_()\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        intermediate_output = model.get_intermediate_layers(inputs, n=1)\n",
        "        features = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
        "        outputs = model.head(features)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        for param in model.parameters():\n",
        "            if param.requires_grad and param.grad is not None:\n",
        "              fisher_scores[param] += (param.grad.data.pow(2) * prev_mask[param])\n",
        "\n",
        "    new_mask = {}\n",
        "    all_scores = torch.cat([torch.flatten(v) for v in fisher_scores.values()])\n",
        "    non_zero_scores=all_scores[all_scores!=0]\n",
        "    k = int(sparsity[round] * non_zero_scores.numel())\n",
        "    threshold, _ = torch.kthvalue(non_zero_scores, non_zero_scores.numel()-k)\n",
        "\n",
        "    for param, score in fisher_scores.items():\n",
        "\n",
        "        masked_score = score * prev_mask[param]\n",
        "        current_mask = ((masked_score < threshold) * prev_mask[param]).float()\n",
        "        new_mask[param] = current_mask\n",
        "        prev_mask[param] = new_mask[param]\n",
        "\n",
        "  return new_mask"
      ],
      "metadata": {
        "id": "-0xQWtZChwz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Self implementation of SGDM with the addition of the mask\n",
        "def sgdm_sparse (params, lr, momentum, dampening, weight_decay, nesterov, maximize,b, mask):\n",
        "    for param in params:\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        grad = param.grad.data\n",
        "\n",
        "        if weight_decay!= 0:\n",
        "          grad=grad+weight_decay*param.data\n",
        "\n",
        "        if param not in b:\n",
        "          b[param] = torch.zeros_like(param.data)\n",
        "\n",
        "        if momentum!=0:\n",
        "            b_toll = b[param]\n",
        "            b_new = momentum * b_toll + (1 - dampening) * grad\n",
        "            if nesterov:\n",
        "               update=grad+momentum*b_new\n",
        "            else:\n",
        "              update=b_new\n",
        "        else:\n",
        "           update=grad\n",
        "           b_new=0\n",
        "\n",
        "        update = update * mask[param]\n",
        "\n",
        "        if maximize:\n",
        "          param.data=param.data+lr*update\n",
        "          b[param] = b_new\n",
        "        else:\n",
        "          param.data=param.data-lr*update\n",
        "          b[param] = b_new\n",
        "    return b"
      ],
      "metadata": {
        "id": "Om_2NPRJhyNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sgd_sparse(epoch, model, inputs, targets, criterion, lr, momentum, weight_decay, mask):\n",
        "     model.train()\n",
        "     running_loss = 0.0\n",
        "     correct = 0\n",
        "     total = 0\n",
        "     params=list(model.parameters())\n",
        "     dampening=0\n",
        "     nesterov=False\n",
        "     maximize=False\n",
        "     b={}\n",
        "\n",
        "     inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "     intermediate_output = model.get_intermediate_layers(inputs, n=1)\n",
        "     features = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
        "     outputs = model.head(features)\n",
        "\n",
        "     loss=criterion(outputs, targets)\n",
        "     model.zero_grad()\n",
        "     loss.backward()\n",
        "\n",
        "     b=sgdm_sparse(params, lr, momentum, dampening, weight_decay, nesterov, maximize,b, mask)"
      ],
      "metadata": {
        "id": "RVY6joQI5UF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse FL with I.I.D. sharding\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "momentum = 0\n",
        "weight_decay = 1e-4\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "lr = 1e-2\n",
        "T=40\n",
        "vits16_mask=copy.deepcopy(vits16_new)\n",
        "FedAvg(vits16_mask, K, C, J, T, lr, momentum, weight_decay,loss_fn, client_loader, trainable_part='head', implementation='pyt', client_loader_mask=client_loader_mask, test_loader=test_loader, test=False)\n",
        "lr = 5e-3\n",
        "T=60\n",
        "test_acc_fed_iid_list, test_loss_fed_iid_list=FedAvg(vits16_mask, K, C, J, T, lr, momentum, weight_decay,loss_fn, client_loader, trainable_part='full', implementation='self', client_loader_mask=client_loader_mask, test_loader=test_loader, test=True)\n",
        "\n",
        "print(f'Validation for i.i.d dataset')\n",
        "validate(vits16_mask, test_loader, loss_fn)\n",
        "print(\"-\"*50)\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "plt.plot(test_loss_fed_iid_list, label='Test Loss')\n",
        "ticks = [0] + [i for i in range(4, T, 5) if i != 0]\n",
        "labels = [1] + [i + 1 for i in range(4, T, 5) if i != 0]\n",
        "plt.xticks(ticks=ticks, labels=labels)\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.title('Test loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vmGmAIKTzKR3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse FL with non I.I.D. balanced sharding for each J and Nc\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = [4,8,16]\n",
        "T=[60, 30, 15]\n",
        "momentum = 0\n",
        "weight_decay = 1e-3\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "for n in range(4):\n",
        "  for j in range(3):\n",
        "    lr = 1e-2\n",
        "    T_head=40\n",
        "    vits16_mask_non_iid_bal=copy.deepcopy(vits16_new)\n",
        "    FedAvg(vits16_mask_non_iid_bal, K, C, J[j], T_head, lr, momentum, weight_decay,loss_fn, client_loader_noniid_balanced_list[n], trainable_part='head', implementation='pyt', client_loader_mask=client_loader_mask_noniid_balanced_list[n], test_loader=test_loader, test=False)\n",
        "\n",
        "    lr = 5e-3\n",
        "    FedAvg(vits16_mask_non_iid_bal, K, C, J[j], T[j], lr, momentum, weight_decay,loss_fn, client_loader_noniid_balanced_list[n], trainable_part='full', implementation='self', client_loader_mask=client_loader_mask_noniid_balanced_list[n], test_loader=test_loader, test=False)\n",
        "\n",
        "    print(f'Validation for non i.i.d.balanced dataset with with N={Nc[n]} and J={J[j]}')\n",
        "    validate(vits16_mask_non_iid_bal, test_loader, loss_fn)\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "dwmDh6ccz8vH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}